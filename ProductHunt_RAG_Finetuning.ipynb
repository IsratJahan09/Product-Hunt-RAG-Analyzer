{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Product Hunt RAG Analyzer - Fine-Tuning with Unsloth + QLoRA\n",
        "\n",
        "This notebook fine-tunes a 3B parameter LLM for Product Hunt competitive analysis tasks:\n",
        "- Product Q&A\n",
        "- Product Comparison\n",
        "- Product Analysis\n",
        "- Feature Gap Detection\n",
        "- Sentiment Analysis\n",
        "\n",
        "**Framework:** Unsloth (2x faster training, 60% less memory)\n",
        "**Method:** QLoRA (4-bit quantization + LoRA adapters)\n",
        "**Model:** Qwen2.5-3B-Instruct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Base directory\n",
        "BASE_DIR = \"/content/drive/MyDrive/ProductHuntRAG\"\n",
        "!mkdir -p \"{BASE_DIR}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Define File Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training data files\n",
        "TRAIN_FILES = [\n",
        "    os.path.join(BASE_DIR, \"train.jsonl\"),\n",
        "]\n",
        "\n",
        "TEST_FILE = os.path.join(BASE_DIR, \"test.jsonl\")  # optional\n",
        "\n",
        "# Quick check that files exist\n",
        "for f in TRAIN_FILES:\n",
        "    if not os.path.exists(f):\n",
        "        print(f\"Missing file: {f}\")\n",
        "        print(\"Please upload your .jsonl files to your ProductHuntRAG folder in Drive first!\")\n",
        "        raise FileNotFoundError(f)\n",
        "print(\"All training files found ‚úì\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Load Qwen 2.5 3B Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"unsloth/Qwen2.5-3B-Instruct\"\n",
        "MAX_SEQ_LEN = 2048\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = MODEL_NAME,\n",
        "    max_seq_length = MAX_SEQ_LEN,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Format Dataset for Chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eos_token = tokenizer.eos_token\n",
        "SYSTEM_PROMPT = \"You are an expert Product Hunt analyst specializing in competitive analysis, feature gap detection, and market positioning insights.\"\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"Convert instruction/input/output format to chat messages format.\"\"\"\n",
        "    texts = []\n",
        "    for i in range(len(examples['instruction'])):\n",
        "        instruction = examples['instruction'][i]\n",
        "        input_data = examples['input'][i]\n",
        "        output_data = examples['output'][i]\n",
        "        \n",
        "        # Build user content with instruction and input context\n",
        "        if isinstance(input_data, dict):\n",
        "            import json\n",
        "            context = json.dumps(input_data, indent=2, ensure_ascii=False)\n",
        "        else:\n",
        "            context = str(input_data)\n",
        "        \n",
        "        user_content = f\"{instruction}\\n\\nProduct Data:\\n{context}\"\n",
        "        \n",
        "        # Extract final answer from output\n",
        "        if isinstance(output_data, dict):\n",
        "            assistant_content = output_data.get('final_answer', json.dumps(output_data))\n",
        "        else:\n",
        "            assistant_content = str(output_data)\n",
        "        \n",
        "        # Create chat messages\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": user_content},\n",
        "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
        "        ]\n",
        "        \n",
        "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "        texts.append(text + eos_token)\n",
        "    \n",
        "    return {\"text\": texts}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Load and Split Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading datasets from Drive...\")\n",
        "full_train = load_dataset(\"json\", data_files=TRAIN_FILES, split=\"train\")\n",
        "\n",
        "# 80/20 split\n",
        "split = full_train.train_test_split(test_size=0.20, seed=3407)\n",
        "train_dataset = split[\"train\"].map(formatting_prompts_func, batched=True)\n",
        "val_dataset = split[\"test\"].map(formatting_prompts_func, batched=True)\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Val size: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir = os.path.join(BASE_DIR, \"outputs\"),\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    num_train_epochs = 3,\n",
        "    learning_rate = 2e-4,\n",
        "    warmup_steps = 5,\n",
        "    logging_steps = 10,\n",
        "    eval_strategy = \"steps\",\n",
        "    eval_steps = 25,\n",
        "    fp16 = not torch.cuda.is_bf16_supported(),\n",
        "    bf16 = torch.cuda.is_bf16_supported(),\n",
        "    optim = \"adamw_8bit\",\n",
        "    report_to = \"none\",\n",
        "    seed = 3407,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = val_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = MAX_SEQ_LEN,\n",
        "    args = training_args,\n",
        ")\n",
        "\n",
        "print(\"\\nStarting fine-tuning now...\\n\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Save Adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ADAPTER_DIR = os.path.join(BASE_DIR, \"adapters_qwen3b\")\n",
        "model.save_pretrained(ADAPTER_DIR)\n",
        "tokenizer.save_pretrained(ADAPTER_DIR)\n",
        "print(f\"\\nDone! Adapters saved in {ADAPTER_DIR}\")\n",
        "print(\"You can now use this model with FastLanguageModel.from_pretrained(..., load_in_4bit=True)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    print(\"CUDA version:\", torch.version.cuda)\n",
        "else:\n",
        "    print(\"No GPU detected by PyTorch!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Evaluation & Loss Curve Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import unsloth\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import json\n",
        "import glob\n",
        "from unsloth import FastLanguageModel\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define Paths\n",
        "BASE_DIR = \"/content/drive/MyDrive/ProductHuntRAG\"\n",
        "ADAPTER_DIR = os.path.join(BASE_DIR, \"adapters_qwen3b\")\n",
        "OUTPUTS_DIR = os.path.join(BASE_DIR, \"outputs\")\n",
        "\n",
        "# ==============================================================\n",
        "# A. Plot Training vs Validation Loss\n",
        "# ==============================================================\n",
        "print(\"üìä Generating Training/Validation Loss Chart...\")\n",
        "\n",
        "history = []\n",
        "\n",
        "# Try from active trainer\n",
        "if 'trainer' in globals() and hasattr(trainer.state, 'log_history'):\n",
        "    history = trainer.state.log_history\n",
        "    print(\"‚úÖ Loaded history from active trainer.\")\n",
        "else:\n",
        "    print(\"üîç Searching for saved logs...\")\n",
        "    checkpoint_dirs = glob.glob(os.path.join(OUTPUTS_DIR, \"checkpoint-*\"))\n",
        "    if checkpoint_dirs:\n",
        "        latest_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n",
        "        log_file = os.path.join(latest_checkpoint, \"trainer_state.json\")\n",
        "        if os.path.exists(log_file):\n",
        "            with open(log_file) as f:\n",
        "                state = json.load(f)\n",
        "            history = state.get('log_history', [])\n",
        "            print(f\"‚úÖ Loaded logs from: {log_file}\")\n",
        "\n",
        "# Plot\n",
        "if history:\n",
        "    steps = []\n",
        "    train_loss = []\n",
        "    val_steps = []\n",
        "    val_loss = []\n",
        "\n",
        "    for entry in history:\n",
        "        if 'loss' in entry:\n",
        "            steps.append(entry.get('step', len(steps)+1))\n",
        "            train_loss.append(entry['loss'])\n",
        "        if 'eval_loss' in entry:\n",
        "            val_steps.append(entry.get('step', len(val_steps)+1))\n",
        "            val_loss.append(entry['eval_loss'])\n",
        "\n",
        "    plt.figure(figsize=(11, 6))\n",
        "    plt.plot(steps, train_loss, label='Training Loss', color='blue', alpha=0.8)\n",
        "    if val_loss:\n",
        "        plt.plot(val_steps, val_loss, label='Validation Loss', color='red', linewidth=2.5, marker='o')\n",
        "\n",
        "    plt.title('ProductHunt RAG Fine-Tuning Learning Curve (Qwen2.5-3B)', fontsize=15)\n",
        "    plt.xlabel('Training Steps')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    chart_path = os.path.join(BASE_DIR, \"producthunt_loss_curve_final.png\")\n",
        "    plt.savefig(chart_path, dpi=200, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"‚úÖ Loss curve saved: {chart_path}\")\n",
        "else:\n",
        "    print(\"‚ùå No training history found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Qualitative Inference Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nü§ñ Loading base model + ProductHunt RAG LoRA adapter...\")\n",
        "\n",
        "BASE_MODEL = \"unsloth/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = BASE_MODEL,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "# Re-create LoRA structure (must match training)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    use_gradient_checkpointing = False,\n",
        ")\n",
        "\n",
        "# Load saved adapter with a name and activate it\n",
        "model.load_adapter(ADAPTER_DIR, adapter_name=\"producthunt\")\n",
        "model.set_adapter(\"producthunt\")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "print(\"‚úÖ ProductHunt RAG model fully loaded and ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test prompts for Product Hunt analysis\n",
        "test_prompts = [\n",
        "    \"What are the key features of Notion and how does it compare to competitors?\",\n",
        "    \"Analyze the sentiment of user reviews for a task management app.\",\n",
        "    \"What feature gaps exist in current AI writing tools?\",\n",
        "    \"Compare Figma vs Sketch for UI design workflows.\",\n",
        "    \"What makes a Product Hunt launch successful?\",\n",
        "    \"Identify potential competitors for a new AI-powered code review tool.\",\n",
        "]\n",
        "\n",
        "results = []\n",
        "print(\"\\nüß† Running inference...\\n\")\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert Product Hunt analyst specializing in competitive analysis, feature gap detection, and market positioning insights.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        use_cache=True,\n",
        "    )\n",
        "\n",
        "    full_text = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n",
        "\n",
        "    # Extract assistant reply (Qwen2.5 safe)\n",
        "    if \"<|im_start|>assistant\" in full_text:\n",
        "        reply = full_text.split(\"<|im_start|>assistant\")[-1]\n",
        "        if \"<|im_end|>\" in reply:\n",
        "            reply = reply.split(\"<|im_end|>\")[0]\n",
        "        reply = reply.strip()\n",
        "    else:\n",
        "        reply = full_text\n",
        "\n",
        "    results.append({\"Prompt\": prompt, \"Response\": reply})\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Response: {reply}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "# Save results\n",
        "df = pd.DataFrame(results)\n",
        "csv_path = os.path.join(BASE_DIR, \"producthunt_final_evaluation.csv\")\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(f\"\\n‚úÖ Final evaluation report saved: {csv_path}\")\n",
        "\n",
        "print(\"\\nüéâ Evaluation complete! Your ProductHunt RAG model is ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Merge LoRA into Full FP16 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/ProductHuntRAG\"\n",
        "ADAPTER_DIR = os.path.join(BASE_DIR, \"adapters_qwen3b\")\n",
        "MERGED_DIR = os.path.join(BASE_DIR, \"merged_producthunt_16bit\")\n",
        "\n",
        "os.makedirs(MERGED_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Loading base model + LoRA adapter...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length = 2048,\n",
        "    dtype = torch.float16,\n",
        "    load_in_4bit = False,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "# Load fine-tuned weights\n",
        "model.load_adapter(ADAPTER_DIR, adapter_name=\"producthunt\")\n",
        "model.set_adapter(\"producthunt\")\n",
        "\n",
        "print(\"Merging LoRA into full model (this takes 5-10 minutes)...\")\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "print(\"Saving full merged FP16 model...\")\n",
        "model.save_pretrained(MERGED_DIR, safe_serialization=True)\n",
        "tokenizer.save_pretrained(MERGED_DIR)\n",
        "\n",
        "print(f\"‚úÖ Full merged model saved to: {MERGED_DIR}\")\n",
        "print(\"Now you can run your GGUF conversion script on this folder!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Convert to GGUF for Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# ==============================================================\n",
        "#  1. Setup & Mount Drive\n",
        "# ==============================================================\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Define Paths\n",
        "project_dir = \"/content/drive/MyDrive/ProductHuntRAG\"\n",
        "INPUT_DIR   = os.path.join(project_dir, \"merged_producthunt_16bit\")\n",
        "FINAL_GGUF  = os.path.join(project_dir, \"GGUF_Models\", \"producthunt_rag_3b_q4_k_m.gguf\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(os.path.dirname(FINAL_GGUF), exist_ok=True)\n",
        "\n",
        "print(f\"üìÇ Input Model: {INPUT_DIR}\")\n",
        "print(f\"üìÇ Output GGUF: {FINAL_GGUF}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "#  2. Build llama.cpp (New CMake Method)\n",
        "# ==============================================================\n",
        "# Clean up any old build to avoid conflicts\n",
        "if os.path.exists(\"llama.cpp\"):\n",
        "    print(\"üßπ Removing old llama.cpp directory...\")\n",
        "    shutil.rmtree(\"llama.cpp\")\n",
        "\n",
        "print(\"‚¨áÔ∏è Cloning llama.cpp...\")\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "\n",
        "# Install dependencies for the conversion script\n",
        "print(\"üì¶ Installing Python dependencies...\")\n",
        "!pip install -r llama.cpp/requirements.txt\n",
        "\n",
        "# Enter directory\n",
        "os.chdir(\"llama.cpp\")\n",
        "\n",
        "print(\"üõ†Ô∏è Building with CMake...\")\n",
        "# Configure and Build\n",
        "!cmake -B build\n",
        "!cmake --build build --config Release -j 4\n",
        "\n",
        "# Verify the binary exists in the new location (build/bin/)\n",
        "QUANTIZE_BIN = \"build/bin/llama-quantize\"\n",
        "\n",
        "if not os.path.exists(QUANTIZE_BIN):\n",
        "    # Fallback check for some environments\n",
        "    if os.path.exists(\"build/bin/Release/llama-quantize\"):\n",
        "        QUANTIZE_BIN = \"build/bin/Release/llama-quantize\"\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"‚ùå Build failed! Could not find '{QUANTIZE_BIN}'.\")\n",
        "\n",
        "print(f\"‚úÖ Build complete. Binary found at: {QUANTIZE_BIN}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "#  3. Convert HF to GGUF (FP16)\n",
        "# ==============================================================\n",
        "print(\"\\n--- üì¶ Converting HF to GGUF (16-bit) ---\")\n",
        "\n",
        "TEMP_GGUF = \"../temp_f16.gguf\"\n",
        "\n",
        "!python3 convert_hf_to_gguf.py \"{INPUT_DIR}\" --outfile \"{TEMP_GGUF}\" --outtype f16\n",
        "\n",
        "if not os.path.exists(TEMP_GGUF):\n",
        "    raise FileNotFoundError(\"‚ùå Conversion failed! Intermediate F16 GGUF was not created.\")\n",
        "\n",
        "print(\"‚úÖ Intermediate F16 GGUF created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "#  4. Quantize to q4_k_m\n",
        "# ==============================================================\n",
        "print(\"\\n--- üìâ Quantizing to q4_k_m ---\")\n",
        "\n",
        "!./{QUANTIZE_BIN} \"{TEMP_GGUF}\" \"{FINAL_GGUF}\" q4_k_m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "#  5. Cleanup & Verify\n",
        "# ==============================================================\n",
        "if os.path.exists(FINAL_GGUF):\n",
        "    print(\"\\n--- üßπ Cleaning up temp file ---\")\n",
        "    os.remove(TEMP_GGUF)\n",
        "\n",
        "    file_size = os.path.getsize(FINAL_GGUF) / (1024 * 1024 * 1024)  # Size in GB\n",
        "    print(f\"üéâ SUCCESS! Final model saved to: {FINAL_GGUF}\")\n",
        "    print(f\"üìä File Size: {file_size:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå Error: Quantization failed. Final file not created.\")\n",
        "\n",
        "# Return to root directory\n",
        "os.chdir(\"..\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You've successfully fine-tuned a 3B model for Product Hunt competitive analysis!\n",
        "\n",
        "**Next Steps:**\n",
        "1. Download the LoRA adapters or GGUF model from Google Drive\n",
        "2. For Ollama: Create a Modelfile and run `ollama create producthunt-rag -f Modelfile`\n",
        "3. Integrate with your Product Hunt RAG Analyzer\n",
        "4. Test with real product queries"
      ]
    }
  ]
}